{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "stm=PorterStemmer() #어근 추출 객체\n",
    "stopwords=set(stopwords.words(\"english\")) #영어 불용어 사전\n",
    "#영문자로 시작하고 영문자,숫자,-,_,. 로 구성된 정규표현식 패턴\n",
    "pattern=re.compile(\"[a-zA-Z][-_a-zA-Z0-9.]*\") \n",
    "\n",
    "def tokenize(sentence):\n",
    "    def stem(w):\n",
    "        try: return stm.stem(w) #어근 추출\n",
    "        except: return w\n",
    "    #소문자로 바꾸고 단어 단위로 토크나이징, 불용어 제거\n",
    "    return [stem(w) for w in word_tokenize(sentence.lower())\n",
    "           if w not in stopwords and pattern.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 1\n",
      "Total words: 162\n",
      "Vocab size: 21\n",
      "Topic #0\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #1\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #2\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #3\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #4\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #5\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #6\tamerican, protect, new, america, nation, countri, peopl, one, everi, make\n",
      "Topic #7\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #8\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #9\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #10\tcountri, great, presid, america, american, nation, peopl, one, protect, everi\n",
      "Topic #11\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #12\tpeopl, everi, never, america, american, nation, countri, one, protect, make\n",
      "Topic #13\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #14\tone, back, today, right, america, american, nation, countri, peopl, protect\n",
      "Topic #15\tworld, across, america, american, nation, countri, peopl, one, protect, everi\n",
      "Topic #16\tamerica, make, dream, mani, american, nation, countri, peopl, one, protect\n",
      "Topic #17\tnation, god, america, presid, american, countri, peopl, one, protect, everi\n",
      "Topic #18\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n",
      "Topic #19\tamerica, american, nation, countri, peopl, one, protect, everi, make, never\n"
     ]
    }
   ],
   "source": [
    "#pip install tomotopy\n",
    "import tomotopy as tp\n",
    "#토픽모델링 객체, 토픽의 갯수 20개, 5회 미만 등장 단어들 제거\n",
    "model=tp.LDAModel(k=20, min_cf=5)\n",
    "for i,line in enumerate(open(\"d:/learn/data/trumph.txt\", encoding=\"ms949\")):\n",
    "    model.add_doc(tokenize(line)) #모델에 한 문장씩 입력\n",
    "    \n",
    "model.train(0)    #0회 학습\n",
    "print(\"Total docs:\",len(model.docs))\n",
    "print(\"Total words:\",model.num_words)\n",
    "print(\"Vocab size:\",model.num_vocabs)\n",
    "\n",
    "model.train(200) #200회 학습\n",
    "for i in range(model.k):\n",
    "    res=model.get_topic_words(i,top_n=10)\n",
    "    print(\"Topic #{0}\".format(i), end=\"\\t\")\n",
    "    print(\", \".join(w for w,p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
